"""
Train a variable-length sequence classifier for PillowMate sensor data.

This script expects JSON files generated by sequence_pipeline/node/collect_sequences.js.
Each file should contain:
  {
    "label": "hug",
    "sample_ms": 20,
    "feature_names": ["pressure_delta","ax","ay","az","gx","gy","gz"],
    "features": [
       [deltaP, ax, ay, az, gx, gy, gz],
       ...
    ]
  }
"""

from __future__ import annotations

import argparse
import json
from collections import Counter
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

import numpy as np
import torch
from sklearn.model_selection import train_test_split
from torch import nn
from torch.utils.data import DataLoader, Dataset

from sequence_model import SequenceGRU


FEATURE_NAMES = ["pressure_delta", "ax", "ay", "az", "gx", "gy", "gz"]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Train a GRU-based sequence classifier.")
    parser.add_argument(
        "--data-dir",
        dest="data_dirs",
        type=Path,
        nargs="+",
        default=[Path(__file__).resolve().parents[1] / "data" / "raw"],
        help="One or more directories containing sequence JSON files.",
    )
    parser.add_argument(
        "--model-out",
        type=Path,
        default=Path(__file__).resolve().parents[1] / "models" / "sequence_classifier.pt",
        help="Where to save the trained PyTorch weights.",
    )
    parser.add_argument(
        "--config-out",
        type=Path,
        default=Path(__file__).resolve().parents[1] / "models" / "sequence_config.json",
        help="Where to save label + normalization metadata.",
    )
    parser.add_argument("--epochs", type=int, default=25, help="Number of training epochs.")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size for DataLoader.")
    parser.add_argument("--hidden-dim", type=int, default=128, help="Hidden dimension for the GRU.")
    parser.add_argument("--learning-rate", type=float, default=1e-3, help="Adam learning rate.")
    parser.add_argument("--val-split", type=float, default=0.2, help="Validation split ratio.")
    parser.add_argument("--random-state", type=int, default=42, help="Deterministic split seed.")
    parser.add_argument("--device", type=str, default="auto", help="cpu, cuda, mps, or auto.")
    parser.add_argument(
        "--low-pass-window",
        type=int,
        default=1,
        help="Apply a moving-average low-pass filter of this window size to every sequence (>=1).",
    )
    parser.add_argument(
        "--stop-when-val-acc",
        type=float,
        default=None,
        help="Early-stop when validation accuracy reaches or exceeds this value (0~1). Requires a validation set.",
    )
    parser.add_argument(
        "--stop-patience",
        type=int,
        default=1,
        help="Number of consecutive epochs meeting --stop-when-val-acc before stopping (default 1).",
    )
    parser.add_argument(
        "--log-misclassifications",
        action="store_true",
        help="Print misclassified label pairs on each validation pass.",
    )
    return parser.parse_args()


def low_pass_filter(sequence: np.ndarray, window: int) -> np.ndarray:
    if window <= 1 or sequence.shape[0] < 2:
        return sequence
    kernel = np.ones(window, dtype=np.float32) / window
    filtered = np.empty_like(sequence)
    for col in range(sequence.shape[1]):
        filtered[:, col] = np.convolve(sequence[:, col], kernel, mode="same")
    return filtered


def load_sequences(data_dirs: Sequence[Path], low_pass_window: int) -> List[Dict]:
    paths: List[Path] = []
    for data_dir in data_dirs:
        paths.extend(sorted(data_dir.glob("*.json")))
    if not paths:
        joined = ", ".join(str(d) for d in data_dirs)
        raise FileNotFoundError(f"No JSON files found in {joined}. Run the sequence collector first.")
    records = []
    for path in paths:
        with path.open() as fp:
            payload = json.load(fp)
        features = payload.get("features", [])
        if not features:
            continue
        if len(features[0]) != len(FEATURE_NAMES):
            raise ValueError(f"{path} feature dimension mismatch. Expected {len(FEATURE_NAMES)} values per frame.")
        records.append(
            {
                "label": payload["label"],
                "features": low_pass_filter(np.array(features, dtype=np.float32), low_pass_window),
                "sample_ms": payload.get("sample_ms", 20),
                "path": str(path),
            }
        )
    if not records:
        raise ValueError("No usable sequences (non-empty features) were found.")
    return records


def compute_feature_stats(records: Sequence[Dict]) -> Tuple[np.ndarray, np.ndarray]:
    total_frames = 0
    feature_sum = np.zeros(len(FEATURE_NAMES), dtype=np.float64)
    feature_sq_sum = np.zeros(len(FEATURE_NAMES), dtype=np.float64)
    for record in records:
        features = record["features"]
        total_frames += features.shape[0]
        feature_sum += features.sum(axis=0)
        feature_sq_sum += (features ** 2).sum(axis=0)
    if total_frames == 0:
        raise ValueError("Cannot compute normalization statistics with zero frames.")
    mean = feature_sum / total_frames
    variance = feature_sq_sum / total_frames - mean**2
    std = np.sqrt(np.clip(variance, 1e-6, None))
    return mean.astype(np.float32), std.astype(np.float32)


class SequenceDataset(Dataset):
    def __init__(self, records: Sequence[Dict], label_to_index: Dict[str, int], mean: np.ndarray, std: np.ndarray):
        self.records = records
        self.label_to_index = label_to_index
        self.mean = torch.from_numpy(mean)
        self.std = torch.from_numpy(std)

    def __len__(self) -> int:
        return len(self.records)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        record = self.records[idx]
        sequence = torch.from_numpy(record["features"])
        sequence = (sequence - self.mean) / self.std
        label_idx = self.label_to_index[record["label"]]
        return sequence, label_idx


def pad_collate(batch):
    sequences, labels = zip(*batch)
    lengths = torch.tensor([seq.shape[0] for seq in sequences], dtype=torch.long)
    feature_dim = sequences[0].shape[1]
    max_len = lengths.max().item()
    padded = torch.zeros(len(sequences), max_len, feature_dim, dtype=torch.float32)
    for i, seq in enumerate(sequences):
        padded[i, : seq.shape[0]] = seq
    labels_tensor = torch.tensor(labels, dtype=torch.long)
    return padded, lengths, labels_tensor


def choose_device(arg: str) -> torch.device:
    if arg == "auto":
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return torch.device(arg)


def train_loop(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    epochs: int,
    stop_threshold: float | None,
    stop_patience: int,
    label_names: List[str],
    log_misclassified: bool,
) -> Dict[str, List[float]]:
    history = {"train_loss": [], "val_loss": [], "val_acc": []}
    patience_counter = 0
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        for sequences, lengths, labels in train_loader:
            sequences = sequences.to(device)
            lengths = lengths.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            logits = model(sequences, lengths)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * sequences.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)
        history["train_loss"].append(epoch_loss)

        if val_loader is not None:
            val_metrics = evaluate(
                model,
                val_loader,
                criterion,
                device,
                label_names,
                log_misclassified,
            )
            history["val_loss"].append(val_metrics["loss"])
            history["val_acc"].append(val_metrics["accuracy"])
            print(
                f"[Epoch {epoch:02d}] "
                f"train_loss={epoch_loss:.4f} val_loss={val_metrics['loss']:.4f} val_acc={val_metrics['accuracy']*100:.2f}%"
            )
            if log_misclassified and val_metrics.get("misclassified"):
                print("  오분류 라벨:")
                for true_label, preds in val_metrics["misclassified"].items():
                    details = ", ".join(f"{pred}->{count}" for pred, count in preds.items())
                    print(f"    {true_label}: {details}")
            if stop_threshold is not None and val_metrics["accuracy"] >= stop_threshold:
                patience_counter += 1
                if patience_counter >= stop_patience:
                    print(
                        f"Validation accuracy {val_metrics['accuracy']*100:.2f}% "
                        f"met threshold {stop_threshold*100:.1f}% for {patience_counter} epoch(s). Stopping early.",
                    )
                    break
            else:
                patience_counter = 0
        else:
            print(f"[Epoch {epoch:02d}] train_loss={epoch_loss:.4f} (validation skipped)")
    return history


def evaluate(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    device: torch.device,
    label_names: Sequence[str],
    log_misclassified: bool,
) -> Dict[str, float]:
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    misclassified: Dict[str, Dict[str, int]] = {label: {} for label in label_names} if log_misclassified else {}
    with torch.no_grad():
        for sequences, lengths, labels in loader:
            sequences = sequences.to(device)
            lengths = lengths.to(device)
            labels = labels.to(device)
            logits = model(sequences, lengths)
            loss = criterion(logits, labels)
            total_loss += loss.item() * sequences.size(0)
            preds = logits.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += sequences.size(0)
            if log_misclassified:
                for true_idx, pred_idx in zip(labels.cpu().tolist(), preds.cpu().tolist()):
                    if true_idx == pred_idx:
                        continue
                    true_label = label_names[true_idx]
                    pred_label = label_names[pred_idx]
                    misclassified.setdefault(true_label, {})
                    misclassified[true_label][pred_label] = misclassified[true_label].get(pred_label, 0) + 1
    avg_loss = total_loss / max(total, 1)
    accuracy = correct / max(total, 1)
    result: Dict[str, float | Dict[str, Dict[str, int]]] = {"loss": avg_loss, "accuracy": accuracy}
    if log_misclassified:
        filtered = {k: v for k, v in misclassified.items() if v}
        result["misclassified"] = filtered
    return result


def save_config(
    config_path: Path,
    labels: List[str],
    mean: np.ndarray,
    std: np.ndarray,
    model_params: Dict[str, int],
) -> None:
    payload = {
        "labels": labels,
        "feature_names": FEATURE_NAMES,
        "feature_mean": mean.tolist(),
        "feature_std": std.tolist(),
        "model": model_params,
    }
    config_path.parent.mkdir(parents=True, exist_ok=True)
    config_path.write_text(json.dumps(payload, indent=2), encoding="utf8")
    print(f"Saved config to {config_path}")


def main() -> None:
    args = parse_args()
    records = load_sequences(args.data_dirs, args.low_pass_window)
    labels = sorted({record["label"] for record in records})
    label_to_index = {label: idx for idx, label in enumerate(labels)}
    mean, std = compute_feature_stats(records)

    label_list = [record["label"] for record in records]
    label_counts = Counter(label_list)
    min_per_class = min(label_counts.values())

    train_records: List[Dict]
    val_records: List[Dict]

    if min_per_class < 2 or len(records) < len(labels) * 2:
        print(
            "Warning: Not enough sequences per class for a stratified validation split. "
            "All sequences will be used for training, and validation metrics will be skipped.",
        )
        train_records = records
        val_records = []
    else:
        record_count = len(records)
        min_val_samples = len(labels)
        desired_val_samples = max(min_val_samples, int(round(record_count * args.val_split)))
        max_val_samples = record_count - len(labels)
        if max_val_samples < min_val_samples:
            raise ValueError(
                "Not enough total sequences to keep at least one validation sample per class. "
                "Collect more data or reduce --val-split.",
            )
        desired_val_samples = min(desired_val_samples, max_val_samples)
        actual_val_ratio = desired_val_samples / record_count

        train_records, val_records = train_test_split(
            records,
            test_size=actual_val_ratio,
            random_state=args.random_state,
            stratify=label_list,
        )
        if not train_records or not val_records:
            raise ValueError("Train/validation split failed. Collect more sequences or adjust --val-split.")

    train_dataset = SequenceDataset(train_records, label_to_index, mean, std)
    val_dataset = SequenceDataset(val_records, label_to_index, mean, std) if val_records else None

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=pad_collate,
    )
    val_loader = (
        DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            collate_fn=pad_collate,
        )
        if val_dataset
        else None
    )
    if args.stop_when_val_acc is not None and val_loader is None:
        print("경고: 검증 세트가 없어 --stop-when-val-acc 옵션을 적용할 수 없습니다.")

    device = choose_device(args.device)
    print(f"Using device: {device}")
    model_kwargs = {
        "feature_dim": len(FEATURE_NAMES),
        "hidden_dim": args.hidden_dim,
        "num_classes": len(labels),
        "num_layers": 2,
        "dropout": 0.1,
    }
    model = SequenceGRU(**model_kwargs).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

    train_loop(
        model,
        train_loader,
        val_loader,
        criterion,
        optimizer,
        device,
        args.epochs,
        args.stop_when_val_acc,
        args.stop_patience,
        labels,
        args.log_misclassifications,
    )

    args.model_out.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), args.model_out)
    print(f"Saved model weights to {args.model_out}")
    save_config(args.config_out, labels, mean, std, model_params=model_kwargs)


if __name__ == "__main__":
    main()
