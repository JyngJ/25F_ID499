// voice_chat_loop.js
// ì„¼ì„œ ì—†ì´ ê³„ì† ëŒ€í™”í•˜ëŠ” PillowMate ë£¨í”„ ë²„ì „

import path from 'path';
import { createTranscription, textToSpeech } from './audio.js';
import { askPillowMate } from './gpt_chat.js';
import { recordAudio } from './recorder.js';
import 'dotenv/config';
import { buildPlaybackCommand, runCommand, getDirname, sleep, checkDependency } from './utils.js'; // Import updated utils
import { config } from './config.js';
import fs from 'fs';

// --------------------------------------------------
const __dirname = getDirname(import.meta.url); // Use getDirname

const INPUT_AUDIO_PATH  = path.join(__dirname, 'assets', 'input.wav');
const OUTPUT_AUDIO_PATH = path.join(__dirname, 'assets', 'reply.mp3');

let conversationHistory = []; // System prompt is now handled by askPillowMate


// --------------------------------------------------
// âœ… í•œ ë²ˆì˜ â€œëŒ€í™” ì‚¬ì´í´â€ë§Œ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜
// --------------------------------------------------
async function handleConversationTurn() {
  // ì´ì „ input.wav íŒŒì¼ ì‚­ì œ
  if (fs.existsSync(INPUT_AUDIO_PATH)) {
    fs.unlinkSync(INPUT_AUDIO_PATH);
  }
  // ë…¹ìŒ
  await recordAudio(INPUT_AUDIO_PATH, {
    startThreshold: parseFloat(config.vad.start_threshold_volume) / 100.0,
    endThreshold: parseFloat(config.vad.end_threshold_volume) / 100.0,
    startThresholdDuration: parseFloat(config.vad.start_threshold_duration),
    minSilenceDuration: parseFloat(config.vad.end_threshold_duration), // Removed * 1000
    maxDuration: parseFloat(config.vad.max_recording_time) // Removed * 1000
  });

  // STT
  console.log('Transcribing...');
  const userText = await createTranscription(INPUT_AUDIO_PATH, 'ko');
  console.log('ğŸ‘¤ User:', userText);

  // ìœ ì € ë§ ë©”ëª¨ì¥ì— ì¶”ê°€
  conversationHistory.push({ role: 'user', content: userText });

  // GPTì—ê²Œ 'ì§€ê¸ˆê¹Œì§€ ëŒ€í™” ì „ì²´'ë¥¼ ë³´ëƒ„
  const gptResponse = await askPillowMate(conversationHistory);
  const replyText = gptResponse.text;
  const action = gptResponse.action;
  const ledPattern = gptResponse.led_pattern;

  // GPT ë‹µë³€ë„ ë©”ëª¨ì¥ì— ì¶”ê°€ (textë§Œ)
  conversationHistory.push({ role: 'assistant', content: replyText });

  console.log('ğŸ§  PillowMate:', replyText);
  console.log('Action:', action);
  console.log('LED Pattern:', ledPattern);


  // TTS
  await textToSpeech(replyText, OUTPUT_AUDIO_PATH);
  await runCommand(buildPlaybackCommand(OUTPUT_AUDIO_PATH));
}


// --------------------------------------------------
// âœ… ê³„ì† ë°˜ë³µë˜ëŠ” ë©”ì¸ ë£¨í”„
// --------------------------------------------------
async function mainLoop() {
  console.log('ğŸ› PillowMate ì‹œì‘ë¨. Ctrl + C ë¡œ ì¢…ë£Œ');

  // ì˜ì¡´ì„± í™•ì¸
  await checkDependency(process.platform === 'win32' ? 'sox' : 'rec', 'brew install sox (macOS) / conda install -c conda-forge sox');


  while (true) {
    console.log('\n----- ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ -----');

    try {
      await handleConversationTurn();
    } catch (err) {
      console.error('âŒ ëŒ€í™” ì¤‘ ì˜¤ë¥˜:', err);
    }

    console.log('â³ 3ì´ˆ íœ´ì‹ í›„ ë‹¤ì‹œ ì‹œì‘...');
    await sleep(3000);
  }
}

mainLoop();
