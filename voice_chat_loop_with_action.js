// voice_chat_loop_with_action.js
// LLM ëŒ€í™”ì™€ ì„¼ì„œ ê¸°ë°˜ í–‰ë™ ì¸ì‹ì„ ë™ì‹œì— ë‹¤ë£¨ëŠ” ë£¨í”„

import path from "path";
import fs from "fs";
import { spawn } from "child_process";
import { createTranscription, textToSpeech } from "./audio.js";
import { askPillowMate } from "./gpt_chat.js";
import { recordAudio, registerLedAdapter } from "./recorder.js";
import { updateSensorDisplay, attachStatusDisplay } from "./status_display.js";
import { buildPlaybackCommand, runCommand, getDirname, sleep, checkDependency } from "./utils.js";
import { config } from "./config.js";

const __dirname = getDirname(import.meta.url);
const INPUT_AUDIO_PATH = path.join(__dirname, "assets", "input.wav");
const OUTPUT_AUDIO_PATH = path.join(__dirname, "assets", "reply.mp3");
const INITIAL_PROMPT = config.initial_prompt;

const ACTION_MODULE_DIR = path.join(__dirname, "ActionRecognitionModule");
const ACTION_NODE_SCRIPT = path.join("node", "run_sequence_inference.js");
const ACTION_MODEL_PATH = "models/sequence_classifier_20251201_more.pt";
const ACTION_CONFIG_PATH = "models/sequence_config_20251201_more.json";

const ACTION_OPTIONS = {
  model: ACTION_MODEL_PATH,
  config: ACTION_CONFIG_PATH,
  lowPassWindow: 5,
  autoIdleArgs: [
    "--auto-idle",
    "--idle-label",
    "idle",
    "--idle-pressure-std",
    "8",
    "--idle-pressure-mean",
    "15",
    "--idle-accel-std",
    "1",
    "--idle-gyro-std",
    "10",
  ],
  pythonDevice: "cpu",
  streamSensors: true,
};

const ACTION_VERBOSE_LOGS = process.env.ACTION_VERBOSE_LOGS === "1";

let sensorDisplayActive = false;

function setSensorDisplayActive(active) {
  sensorDisplayActive = active;
  if (!active) {
    updateSensorDisplay("ðŸ§­ ì„¼ì„œ ëŒ€ê¸° ì¤‘...");
  }
}

class ConsoleLedAdapter {
  setState({ brightness }) {
    // í•˜ë“œì›¨ì–´ LEDê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë”ë¯¸ ì–´ëŒ‘í„°. ì½˜ì†” ì¶œë ¥ì€ ìƒëžµ.
    return brightness;
  }
}

/**
 * ActionRecognizer
 * - run_sequence_inference.jsë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•´ ì„¼ì„œ ê¸°ë°˜ í–‰ë™ ë¼ë²¨ì„ ì–»ëŠ”ë‹¤.
 * - startTurn() í˜¸ì¶œ ì‹œ ì„¼ì„œ ìˆ˜ì§‘ì„ ì‹œìž‘í•˜ê³ , stopAndGetAction()ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°›ëŠ”ë‹¤.
 */
class ActionRecognizer {
  constructor(options) {
    this.cwd = options.cwd;
    this.args = [
      ACTION_NODE_SCRIPT,
      "--model",
      options.model,
      "--config",
      options.config,
      "--low-pass-window",
      String(options.lowPassWindow),
      "--python-device",
      options.pythonDevice,
      "--quiet",
      ...options.autoIdleArgs,
    ];
    if (options.streamSensors) {
      this.args.push("--stream-sensors");
    }
    this.child = spawn("node", this.args, {
      cwd: this.cwd,
      stdio: ["pipe", "pipe", "pipe"],
      env: process.env,
    });
    this.buffer = "";
    this.ready = false;
    this.canStart = false;
    this.pendingResolve = null;
    this.readyPromise = new Promise((resolve) => {
      this.resolveReady = resolve;
    });

    this.child.stdout.setEncoding("utf8");
    this.child.stdout.on("data", (chunk) => this.handleData(chunk));
    this.child.stderr.setEncoding("utf8");
    this.child.stderr.on("data", (chunk) => {
      process.stderr.write(`[action log] ${chunk}`);
    });
    this.child.on("exit", (code, signal) => {
      console.log(`ðŸ›‘ Action recognizer exited (code=${code}, signal=${signal})`);
    });

    process.on("exit", () => this.dispose());
    process.on("SIGINT", () => {
      this.dispose();
      process.exit(0);
    });
  }

  dispose() {
    if (this.child && !this.child.killed) {
      this.child.kill();
    }
  }

  handleData(chunk) {
    this.buffer += chunk;
    const lines = this.buffer.split("\n");
    this.buffer = lines.pop() ?? "";
    for (const line of lines) {
      if (ACTION_VERBOSE_LOGS) {
        console.log(`[action] ${line}`);
      }
      this.processLine(line.trim());
    }
  }

  processLine(line) {
    if (!line) return;
    if (!this.ready && line.includes("ì‚¬ìš©ìž í„´ì„ ë…¹í™”í•˜ë ¤ë©´ Enter")) {
      this.ready = true;
      this.canStart = true;
      this.resolveReady?.();
      return;
    }
    if (line.startsWith("[sensor]")) {
      if (sensorDisplayActive) {
        const data = parseSensorLine(line);
        renderSensorMeters(data);
      }
      return;
    }
    if (line.includes("ìƒˆ í„´ì„ ì‹œìž‘í•˜ë ¤ë©´ Enter")) {
      this.canStart = true;
      return;
    }
    if (this.pendingResolve && line.includes("ì˜ˆì¸¡ ê²°ê³¼:")) {
      const parsed = this.parsePrediction(line);
      const resolver = this.pendingResolve;
      this.pendingResolve = null;
      this.canStart = false;
      resolver(parsed);
      return;
    }
  }

  parsePrediction(line) {
    const regex = /ì˜ˆì¸¡ ê²°ê³¼:\s+([^(]+)\(([\d.]+)%\)/;
    const match = line.match(regex);
    if (!match) {
      return { label: "unknown", probability: 0, raw: line };
    }
    return {
      label: match[1].trim(),
      probability: Number(match[2]) / 100,
      raw: line,
    };
  }

  async ensureReady() {
    await this.readyPromise;
  }

  async startTurn() {
    await this.ensureReady();
    if (!this.canStart) {
      throw new Error("Action recognizer is not ready to start a turn.");
    }
    this.canStart = false;
    this.child.stdin.write("\n");
  }

  async stopAndGetAction() {
    return new Promise((resolve, reject) => {
      const timeout = setTimeout(() => {
        if (this.pendingResolve) {
          this.pendingResolve = null;
          this.canStart = true;
          reject(new Error("Action recognition timed out."));
        }
      }, 15000);
      this.pendingResolve = (result) => {
        clearTimeout(timeout);
        this.canStart = true;
        resolve(result);
      };
      this.child.stdin.write("\n");
    });
  }
}

function parseSensorLine(line) {
  const payload = {};
  line
    .replace("[sensor]", "")
    .trim()
    .split(/\s+/)
    .forEach((part) => {
      const [key, value] = part.split("=");
      if (key) {
        payload[key] = Number(value);
      }
    });
  return payload;
}

function meterBar(value, max, width = 12) {
  const ratio = Math.max(0, Math.min(1, Math.abs(value) / max));
  const filled = Math.round(ratio * width);
  return `${"â–ˆ".repeat(filled).padEnd(width, " ")}`;
}

function renderSensorMeters(data) {
  const dp = data.dp ?? 0;
  const accelMag = Math.sqrt((data.ax ?? 0) ** 2 + (data.ay ?? 0) ** 2 + (data.az ?? 0) ** 2);
  const gyroMag = Math.sqrt((data.gx ?? 0) ** 2 + (data.gy ?? 0) ** 2 + (data.gz ?? 0) ** 2);
  const line = `ðŸ§­ Î”P ${dp.toFixed(1)}  ACC ${accelMag.toFixed(2)}  GYRO ${gyroMag.toFixed(2)}`;
  updateSensorDisplay(`ðŸ§­ ${line}`);
}

let actionRecognizer = null;

function createActionRecognizer() {
  if (actionRecognizer) {
    actionRecognizer.dispose();
  }
  actionRecognizer = new ActionRecognizer({
    cwd: ACTION_MODULE_DIR,
    model: ACTION_OPTIONS.model,
    config: ACTION_OPTIONS.config,
    lowPassWindow: ACTION_OPTIONS.lowPassWindow,
    autoIdleArgs: ACTION_OPTIONS.autoIdleArgs,
    pythonDevice: ACTION_OPTIONS.pythonDevice,
    streamSensors: ACTION_OPTIONS.streamSensors,
  });
  return actionRecognizer;
}

async function resetActionRecognizer() {
  createActionRecognizer();
  await actionRecognizer.ensureReady();
}

let conversationHistory = [];

/**
 * ì‚¬ìš©ìž í•œ í„´ì„ ì²˜ë¦¬í•œë‹¤.
 * 1) ì„¼ì„œ ìˆ˜ì§‘ì„ ì‹œìž‘í•˜ê³  ë…¹ìŒì„ ì§„í–‰
 * 2) STT ê²°ê³¼ì™€ í–‰ë™ ë¼ë²¨ì„ GPT ìž…ë ¥ì— í¬í•¨
 * 3) LLM ì‘ë‹µê³¼ TTS ìž¬ìƒ
 */
async function handleConversationTurn() {
  if (fs.existsSync(INPUT_AUDIO_PATH)) {
    fs.unlinkSync(INPUT_AUDIO_PATH);
  }

  console.log("\nðŸŽ¯ í–‰ë™ ì¸ì‹ ì„¼ì„œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤. ì‚¬ìš©ìž ë°œí™”ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...");
  await actionRecognizer.startTurn();
  setSensorDisplayActive(false);

  try {
    await recordAudio(INPUT_AUDIO_PATH, {
      startThreshold: parseFloat(config.vad.start_threshold_volume) / 100.0,
      endThreshold: parseFloat(config.vad.end_threshold_volume) / 100.0,
      startThresholdDuration: parseFloat(config.vad.start_threshold_duration),
      minSilenceDuration: parseFloat(config.vad.end_threshold_duration),
      maxDuration: parseFloat(config.vad.max_recording_time),
      onSpeechStart: () => setSensorDisplayActive(true),
    });
  } catch (err) {
    // ë…¹ìŒ ì‹¤íŒ¨ ì‹œ ì•¡ì…˜ ì¸ì‹ê¸° ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê³  ìž¬ì‹œë„í•  ìˆ˜ ìžˆê²Œ ì˜¤ë¥˜ ì „íŒŒ
    await resetActionRecognizer();
    throw err;
  }

  const actionPromise = actionRecognizer
    .stopAndGetAction()
    .catch(() => ({ label: "idle", probability: 0, raw: "timeout" }));

  console.log("Transcribing...");
  const userText = await createTranscription(INPUT_AUDIO_PATH, "ko");
  console.log("ðŸ‘¤ User:", userText);

  const actionResult = await actionPromise;
  setSensorDisplayActive(false);
  console.log(
    `ðŸ“Ÿ Detected action: ${actionResult.label} (${(actionResult.probability * 100).toFixed(1)}%)`,
  );

  const userAugmentedText = `${userText}\n\n[Detected action: ${actionResult.label} (${
    (actionResult.probability * 100).toFixed(1)
  }%)]`;
  conversationHistory.push({ role: "user", content: userAugmentedText });

  const gptResponse = await askPillowMate(conversationHistory);
  const replyText = gptResponse.text;
  const action = gptResponse.action;
  const ledPattern = gptResponse.led_pattern;

  conversationHistory.push({ role: "assistant", content: replyText });

  console.log("ðŸ§  PillowMate:", replyText);
  console.log("Action:", action);
  console.log("LED Pattern:", ledPattern);

  await textToSpeech(replyText, OUTPUT_AUDIO_PATH);
  await runCommand(buildPlaybackCommand(OUTPUT_AUDIO_PATH));
}

async function mainLoop() {
  console.log("ðŸ› PillowMate + Action Recognition ì‹œìž‘ë¨. Ctrl + C ë¡œ ì¢…ë£Œ");
  await checkDependency(
    process.platform === "win32" ? "sox" : "rec",
    "brew install sox (macOS) / conda install -c conda-forge sox",
  );

  // ì•¡ì…˜ ì¸ì‹ê¸° ì´ˆê¸°í™”
  createActionRecognizer();

  attachStatusDisplay();
  registerLedAdapter(new ConsoleLedAdapter());
  setSensorDisplayActive(false);
  await actionRecognizer.ensureReady();

  try {
    await textToSpeech(INITIAL_PROMPT, OUTPUT_AUDIO_PATH);
  } catch (e) {
    console.log("TTS Skip:", e.message);
  }

  conversationHistory.push({ role: "assistant", content: INITIAL_PROMPT });
  console.log("PillowMate:", INITIAL_PROMPT);
  await runCommand(buildPlaybackCommand(OUTPUT_AUDIO_PATH));

  while (true) {
    console.log("\n----- ìƒˆë¡œìš´ ëŒ€í™” ì‹œìž‘ -----");
    try {
      await handleConversationTurn();
    } catch (err) {
      console.error("âŒ ëŒ€í™” ì¤‘ ì˜¤ë¥˜:", err);
      // ì•¡ì…˜ ì¸ì‹ê¸° ì¤€ë¹„ ì˜¤ë¥˜ê°€ ë°˜ë³µë  ë•Œ ìž¬ê¸°ë™
      await resetActionRecognizer();
    }
    console.log("â³ 3ì´ˆ íœ´ì‹ í›„ ë‹¤ì‹œ ì‹œìž‘...");
    await sleep(3000);
  }
}

mainLoop().catch((err) => {
  console.error(err);
  process.exit(1);
});
